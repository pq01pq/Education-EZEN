# -*- coding: utf-8 -*-
"""CNN 수업.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19arwZKcBNoHKcDORWxZQD-Lw1cSrfEgr
"""

import tensorflow as tf

!pip uninstall tensorflow

!pip install tensorflow==2.0.0

import tensorflow as tf

from keras.datasets import mnist

(x_train_data, y_train_data), (x_test_data, y_test_data) = mnist.load_data()

x_train = x_train_data
x_test = x_test_data
y_train = y_train_data
y_test = y_test_data

print("x_train shape:", x_train.shape, "y_train shape:", y_train.shape)

import matplotlib.pyplot as plt

plt.imshow(x_train[300])

x_train = x_train.reshape((60000, 28, 28, 1))
x_test = x_test.reshape((10000, 28, 28, 1))

x_train = x_train.astype('float32')/255
x_test = x_test.astype('float32')/255

#모델 만들기

model = tf.keras.models.Sequential([
     tf.keras.layers.Conv2D(filters = 32, strides = (1,1), kernel_size = (3,3), activation='relu'),
     tf.keras.layers.MaxPooling2D((2,2)),
     tf.keras.layers.Conv2D(filters = 64, strides = (1,1), kernel_size = (3,3), activation='relu'),
     tf.keras.layers.MaxPooling2D((2,2)),
     tf.keras.layers.Conv2D(filters = 64, strides = (1,1), kernel_size = (3,3), activation='relu'),
     tf.keras.layers.Flatten(),
     tf.keras.layers.Dense(64, activation = tf.nn.sigmoid),
     tf.keras.layers.Dense(10, activation = tf.nn.softmax)                             
])

#모델 컴파일
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#adam이라는건 momentum 과 adagrad를 섞은 기법입니다
#momentum은 sgd를 보완해주는거, 그리고 이 기울기 업데이트시 폭을 조절하는 역할
#adagrad 같은 경우에는 학습률과 학습 시간 사이의 관계 속에서 절충안

#모델 훈련
model.fit(x_train, y_train, epochs=3)

#정확도 평가

test_loss, test_accuracy = model.evaluate(x_test, y_test)
print('정확도 : ', test_accuracy)

